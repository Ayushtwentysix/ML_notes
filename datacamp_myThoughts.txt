          KNN ALGO
one child asking for the same dimention/color of ball as his friends have. So they will be grouped same. 

now I want to know what color of ball will a kid like if i want to give him surprise on his birthday.
 So for this I will consider various attributes of other 5 children(Binary FEATURES) 
like [ball baught by their parents,take ball to school,ball to be shared by brother/sister etc.]  


US voting culture:
agriculture  education  Rockets  Party 
	0   	   1            1      Democratic
    1          0            1       Republic
    1	       1            1       Republic


Birthday Ball surprise:
bought by parents   take to school      ball shared       color
    1                  0                    1               red
	0                  1                    1               blue
    1                  0                    0               blue	
	
process:                 

[ df.keys() : agriculture,education,rockets,Party]

import various packages                                                             
								from sklearn.neighbors import KNeighborsClassifier
								from sklearn.model_selection import train_test_split
													
													
make target & features arrays
								y = df['party'].values 
								X = df.drop('party', axis=1).values
													 
Split into training and test set
								X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)



Create a k-NN classifier with 6 neighbors	
								knn = KNeighborsClassifier(n_neighbors = 6)
Fit the classifier to the data
								knn.fit(X_train,y_train)
Predict and print the label for the new data point X_new
								new_prediction = knn.predict(X_train)
								print("Prediction: {}".format(new_prediction))
													
Print the accuracy
								print(knn.score(X_test, y_test))
													
													
PLOTTING OVERFITTING AND UNDERFITTING
# Setup arrays to store train and test accuracies
								neighbors = np.arange(1, 9)
								train_accuracy = np.empty(len(neighbors))
								test_accuracy = np.empty(len(neighbors))

# Loop over different values of k
                                for i, k in enumerate(neighbors):
# Setup a k-NN Classifier with k neighbors: knn
                                knn = KNeighborsClassifier(n_neighbors=k)

# Fit the classifier to the training data
                                knn.fit(X_train,y_train)
														
#Compute accuracy on the training set
                                train_accuracy[i] = knn.score(X_train, y_train)

#Compute accuracy on the testing set
                                test_accuracy[i] = knn.score(X_test, y_test)

# Generate plot
								plt.title('k-NN: Varying Number of Neighbors')
								plt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')
								plt.plot(neighbors, train_accuracy, label = 'Training Accuracy')
								plt.legend()
								plt.xlabel('Number of Neighbors')
								plt.ylabel('Accuracy')
								plt.show()

													

													
process for birthday party:												
from sklearn.neighbors import KNeighborsClassifier
y = df['color'].values
X = df.drop('color', axis=1).values	
knn = KNeighborsClassifier(n_neighbors = 6)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)
knn.fit(X_train,y_train)
new_prediction = knn.predict(X_train)
print("Prediction: {}".format(new_prediction))
print(knn.score(X_test, y_test))	

PLOTTING OVERFITTING AND UNDERFITTING   
train_accuracy = np.empty(len(neighbors))	
test_accuracy = np.empty(len(neighbors))	
for i, k in enumerate(neighbors):	
				knn = KNeighborsClassifier(n_neighbors=k)
				X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)
				knn.fit(X_train,y_train)
				train_accuracy[i] = knn.score(X_train, y_train)
				test_accuracy[i] = knn.score(X_test, y_test)
				plt.title('k-NN: Varying Number of Neighbors')
plt.title('k-NN: Varying Number of Neighbors')
plt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')
plt.plot(neighbors, train_accuracy, label = 'Training Accuracy')
plt.legend()
plt.xlabel('Number of Neighbors')
plt.ylabel('Accuracy')
plt.show()															
		
		
		
 supervised learning models 
 Exploratory data analysis (EDA)
 
 
                      REGRESSION- TO SHOW A RELATIONSHIP B/W DIFFERENT ENTITIES(correlation)
first we need to have data in form needed by scikit-learn. This involves creating feature and target variable arrays. since i am using one feature, so I 
have to reshape it, to make it useable. 

fertility education money life
  34		45		45		43
  36        34      78      89
  76         98      67     70
  
  now i want to get a correlation b/w fertility & life: 
  for this i will first make feature array(fertility) and target array(life). Import necessary packages. create a regressor like knn. create prediction space.
 fit the model with given arrays. Compute predictions over the prediction space.Plot regression line  
 
 # Import LinearRegression
											from sklearn.linear_model import LinearRegression

# Create the regressor: reg
											reg = LinearRegression()

# Create the prediction space
											prediction_space = np.linspace(min(X_fertility), max(X_fertility)).reshape(-1,1)

# Fit the model to the data
											reg.fit(X_fertility,y)

# Compute predictions over the prediction space: y_pred
											y_pred = reg.predict(prediction_space)

# Print R^2 
											print(reg.score(X_fertility, y))

# Plot regression line
											plt.plot(prediction_space, y_pred, color='black', linewidth=3)
											plt.show()

	
TRAIN/TEST SPLIT FOR REGRESSION
	
# Import necessary modules
											from sklearn.linear_model import LinearRegression
											from sklearn.metrics import mean_squared_error
											from sklearn.model_selection import train_test_split
											import matplotlib.pyplot as plt

# Create training and test sets
											X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.3, random_state=42)

# Create the regressor: reg_all
											reg_all = LinearRegression()

# Fit the regressor to the training data
											reg_all.fit(X_train,y_train)

# Predict on the test data: y_pred
											y_pred = reg_all.predict(X_test)
	

# Compute and print R^2 and RMSE
											print("R^2: {}".format(reg_all.score(X_test, y_test)))
											a= mean_squared_error(y_test,y_pred)
											rmse = np.sqrt(a)
											print("Root Mean Squared Error: {}".format(rmse))
 

 
 